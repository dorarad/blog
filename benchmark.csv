Timestamp,Email Address,Main Conference / Datasets Track / Workshop? (please specify its name),Title,"Authors (full name, comma separated)",Abstract,key words,Point of contact email address,Image To Represent Paper,"Are you presenting a poster? If so, what's the Poster session date and time? (Format: Dec 1, 17:00-20:00)

[Skip if information is not available and we will contact you close to the conference]","Are you presenting a poster? If so, what's the Poster session number and location? (format: Poster 321, room 123)

[Skip if information is not available and we will contact you close to the conference]","Are you presenting a talk? If so, what's the oral session date and time? (Format: Dec 1, 10:00-13:00)

[Skip if information is not available and we will contact you close to the conference]","Are you presenting a talk? If so, what's the talk session name and location? (format: Generative models, room 123)

[Skip if information is not available and we will contact you close to the conference]","Award Nominations (if any, comma separated)",Link to project website,Link to paper,Link to blog post (if any),"Link to public video (e.g. YouTube, if any)"
11/4/2022 16:39:48,phend@cs.stanford.edu,Datasets and Benchmarks Track,Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset,"Peter Henderson*, Mark S. Krass*, Lucia Zheng, Neel Guha, Christopher D. Manning, Dan Jurafsky, Daniel E. Ho","One concern with the rise of large language models lies with their potential for significant harm, particularly from pretraining on biased, obscene, copyrighted, and private information. Emerging ethical approaches have attempted to filter pretraining material, but such approaches have been ad hoc and failed to take into account context. We offer an approach to filtering grounded in law, which has directly addressed the tradeoffs in filtering material. First, we gather and make available the Pile of Law, a 256GB (and growing) dataset of open-source English-language legal and administrative data, covering court opinions, contracts, administrative rules, and legislative records. Pretraining on the Pile of Law may potentially help with legal tasks that have the promise to improve access to justice. Second, we distill the legal norms that governments have developed to constrain the inclusion of toxic or private content into actionable lessons for researchers and discuss how our dataset reflects these norms. Third, we show how the Pile of Law offers researchers the opportunity to learn such filtering rules directly from the data, providing an exciting new research direction in model-based processing. ","data filtering, responsible ai, law and ai, foundation models",phend@cs.stanford.edu,https://drive.google.com/open?id=1WGMNAcxD4UvgLyvBbxGUEQtBBwsUNIaF,,,,,,https://huggingface.co/datasets/pile-of-law/pile-of-law,https://arxiv.org/abs/2207.00220,,
11/4/2022 18:04:44,atamkin@stanford.edu,Datasets and Benchmarks Track,DABS 2.0: Improved Datasets and Algorithms for Universal Self-Supervision ,"Alex Tamkin, Gaurab Banerjee, Mohamed Owda, Vincent Liu, Shashank Rammoorthy, Noah Goodman","Universal self-supervised (SSL) algorithms hold enormous promise for making machine learning accessible to high-impact domains such as protein biology, manufacturing, and genomics. We present DABS 2.0: a set of improved datasets and algorithms for advancing research on universal SSL. We extend the recently-introduced DABS benchmark with the addition of five real-world science and engineering domains: protein biology, bacterial genomics, multispectral satellite imagery, semiconductor wafers, and particle physics, bringing the total number of domains in the benchmark to twelve. We also propose a new universal SSL algorithm, Capri, and a generalized version of masked autoencoding, and apply both on all twelve domains---the most wide-ranging exploration of SSL yet. We find that multiple algorithms show gains across domains, outperforming previous baselines. In addition, we demonstrate the usefulness of DABS for scientific study of SSL by investigating the optimal corruption rate for each algorithm, showing that the best setting varies based on the domain.","self-supervised learning, domain agnostic, ML for science",atamkin@stanford.edu,https://drive.google.com/open?id=1G5TFo09gAUak11IFjPWILg0OQk6LYGgN,,,,,,https://dabs.stanford.edu/,https://openreview.net/pdf?id=ChWf1E43l4,,
11/7/2022 8:43:43,chengxuz@mit.edu,Datasets and Benchmarks Track,How Well Do Unsupervised Learning Algorithms Model Human Real-time and Life-long Learning?,"Chengxu Zhuang, Violet Xiang, Yoon Bai, Xiaoxuan Jia, Nick Turk-Browne,  Kenneth Norman, James J. DiCarlo, Daniel LK Yamins","Humans learn from visual inputs at multiple timescales, both rapidly and flexibly acquiring visual knowledge over short periods, and robustly accumulating online learning progress over longer periods. Modeling these powerful learning capabilities is an important problem for computational visual cognitive science, and models that could replicate them would be of substantial utility in real-world computer vision settings. In this work, we establish benchmarks for both real-time and life-long continual visual learning. Our real-time learning benchmark measures a model's ability to match the rapid visual behavior changes of real humans over the course of minutes and hours, given a stream of visual inputs. Our life-long learning benchmark evaluates the performance of models in a purely online learning curriculum obtained directly from child visual experience over the course of years of development. We evaluate a spectrum of recent deep self-supervised visual learning algorithms on both benchmarks, finding that none of them perfectly match human performance, though some algorithms perform substantially better than others. Interestingly, algorithms embodying recent trends in self-supervised learning -- including BYOL, SwAV and MAE -- are substantially worse on our benchmarks than an earlier generation of self-supervised algorithms such as SimCLR and MoCo-v2. We present analysis indicating that the failure of these newer algorithms is primarily due to their inability to handle the kind of sparse low-diversity datastreams that naturally arise in the real world, and that actively leveraging memory through negative sampling -- a mechanism eschewed by these newer algorithms -- appears useful for facilitating learning in such low-diversity environments. We also illustrate a complementarity between the short and long timescales in the two benchmarks, showing how requiring a single learning algorithm to be locally context-sensitive enough to match real-time learning changes while stable enough to avoid catastrophic forgetting over the long term induces a trade-off that human-like algorithms may have to straddle. Taken together, our benchmarks establish a quantitative way to directly compare learning between neural networks models and human learners, show how choices in the mechanism by which such algorithms handle sample comparison and memory strongly impact their ability to match human learning abilities, and expose an open problem space for identifying more flexible and robust visual self-supervision algorithms. ","human visual learning, human-like curriculum, real-time visual learning, life-long visual learning, unsupervised learning",chengxuz@mit.edu,https://drive.google.com/open?id=1ThhUn-qX0GLVRwBkDQ1CvMvcvNmNdqCN,,,,,,https://github.com/neuroailab/VisualLearningBenchmarks,https://openreview.net/forum?id=c0l2YolqD2T,,
11/7/2022 10:06:45,joycj@stanford.edu,Datasets and Benchmarks Track,Geoclidean: Few-Shot Generalization in Euclidean Geometry,"Joy Hsu, Jiajun Wu, Noah Goodman","Euclidean geometry is among the earliest forms of mathematical thinking. While the geometric primitives underlying its constructions, such as perfect lines and circles, do not often occur in the natural world, humans rarely struggle to perceive and reason with them. Will computer vision models trained on natural images show the same sensitivity to Euclidean geometry? Here we explore these questions by studying few-shot generalization in the universe of Euclidean geometry constructions. We introduce Geoclidean, a domain-specific language for Euclidean geometry, and use it to generate two datasets of geometric concept learning tasks for benchmarking generalization judgements of humans and machines. We find that humans are indeed sensitive to Euclidean geometry and generalize strongly from a few visual examples of a geometric concept. In contrast, low-level and high-level visual features from standard computer vision models pretrained on natural images do not support correct generalization. Thus Geoclidean represents a novel few-shot generalization benchmark for geometric concept learning, where the performance of humans and of AI models diverge. The Geoclidean framework and dataset are publicly available for download.","geometry, concept learning, few-shot generalization",joycj@stanford.edu,https://drive.google.com/open?id=1ZujvCl3R1NPS8t8jcRhkwTziiWY_se-b,,,,,,http://web.stanford.edu/~joycj/projects/geoclidean_neurips_2022.html,https://openreview.net/pdf?id=3lk54yE2tYJ,,
11/8/2022 14:57:44,xikunz2@cs.stanford.edu,Datasets and Benchmarks Track,CLEVRER-Humans: Describing Physical and Causal Events the Human Way,"Jiayuan Mao*, Xuelin Yang*, Xikun Zhang, Noah Goodman, Jiajun Wu","Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world. However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of the causal relationships. This design brings up two issues. First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments. To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels. We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models. We convert the collected CEGs into questions and answers to be consistent with prior work. Finally, we study a collection of baseline approaches for CLEVRER-Humans question-answering, highlighting great challenges set forth by our benchmark.","video reasoning, causal reasoning, video question answering",jiayuanm@mit.edu,https://drive.google.com/open?id=1VkOXOK0-wNeSd7g5QSz3Or8gpXn1ds_g,,,,,,https://sites.google.com/stanford.edu/clevrer-humans/home,https://openreview.net/pdf?id=sd1fv0g3UO1,,
11/21/2022 13:43:20,lingjiao@stanford.edu,Datasets and Benchmarks Track,HAPI: A Large-scale Longitudinal Dataset of Commercial ML API Predictions,"Lingjiao Chen, Matei Zaharia, James Zou","Commercial ML APIs offered by providers such as Google, Amazon and Microsoft have dramatically simplified ML adoption in many applications. Numerous companies and academics pay to use ML APIs for tasks such as object detection, OCR and sentiment analysis. Different ML APIs tackling the same task can have very heterogeneous performance. Moreover, the ML models underlying the APIs also evolve over time. As ML APIs rapidly become a valuable marketplace and a widespread way to consume machine learning, it is critical to systematically study and compare different APIs with each other and to characterize how APIs change over time. However, this topic is currently underexplored due to the lack of data. In this paper, we present HAPI (History of APIs), a longitudinal dataset of 1,761,417 instances of commercial ML API applications (involving APIs from Amazon, Google, IBM, Microsoft and other providers) across diverse tasks including image
tagging, speech recognition and text mining from 2020 to 2022. Each instance consists of a query input for an API (e.g., an image or text) along with the API’s output prediction/annotation and confidence scores. HAPI is the first large-scale dataset of ML API usages and is a unique resource for studying ML-as-a-service (MLaaS). As examples of the types of analyses that HAPI enables, we show that ML APIs’ performance change substantially over time—several APIs’ accuracies dropped on specific benchmark datasets. Even when the API’s aggregate performance stays steady, its error modes can shift across different subtypes of data between 2020 and 2022. Such changes can substantially impact the entire analytics pipelines that use some ML API as a component. We further use HAPI to study commercial APIs’ performance disparities across demographic subgroups over time. HAPI can stimulate more research in the growing field of MLaaS.","model update, performance shift, ML API, benchmark",lingjiao@stanford.edu,https://drive.google.com/open?id=13hy-cO7g-wI3yXM7Yl4tHuoqWSu8AJ5E,Dec 1 11:30— 13:00 CT,Hall J #1019,,,,http://hapi.stanford.edu/,https://openreview.net/pdf?id=CZeIOfCjMf,,
11/25/2022 19:49:24,rcwang@stanford.edu,Datasets and Benchmarks Track,IKEA-Manual: Seeing Shape Assembly Step by Step,"Ruocheng Wang, Yunzhi Zhang, Jiayuan Mao, Ran Zhang, Chin-Yi Cheng, Jiajun Wu","Human-designed visual manuals are crucial components in shape assembly activities. They provide step-by-step guidance on how we should move and connect different parts in a convenient and physically-realizable way. While there has been an ongoing effort in building agents that perform assembly tasks, the information in human-design manuals has been largely overlooked. We identify that this is due to 1) a lack of realistic 3D assembly objects that have paired manuals and 2) the difficulty of extracting structured information from purely image-based manuals. Motivated by this observation, we present IKEA-Manual, a dataset consisting of 102 IKEA objects paired with assembly manuals. We provide fine-grained annotations on the IKEA objects and assembly manuals, including decomposed assembly parts, assembly plans, manual segmentation, and 2D-3D correspondence between 3D parts and visual manuals. We illustrate the broad application of our dataset on four tasks related to shape assembly: assembly plan generation, part segmentation, pose estimationand 3D part assembly.","computer vision, compositional 3d modeling, assembly",rcwang@stanford.edu,https://drive.google.com/open?id=1gAAuqfyypJ1WqRhMZNkBjTFDyNxAU2JS,"Nov 30, 11:00-13:00 CST",Poster Session 3,,,,,https://cs.stanford.edu/~rcwang/projects/ikea_manual/,,