Timestamp,Email Address,Title,"Authors (full name, comma separated)",Abstract,key words,Point of contact email address,Image To Represent Paper,"Award Nominations (if any, comma separated)","Venue (main conference / a particular workshop / etc). If the paper is presented in a workshop, please list the workshop name.",Link to project website,Link to paper,Link to blog post (if any),"Link to public video (e.g. YouTube, if any)"
6/26/2022 23:16:02,xie@cs.stanford.edu,"Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation","Kendrick Shen*, Robbie Jones,* Ananya Kumar*, Sang Michael Xie*, Jeff Z. HaoChen, Tengyu Ma, Percy Liang","We consider unsupervised domain adaptation (UDA), where labeled data from a source domain (e.g., photographs) and unlabeled data from a target domain (e.g., sketches) are used to learn a classifier for the target domain. Conventional UDA methods (e.g., domain adversarial training) learn domain-invariant features to improve generalization to the target domain. In this paper, we show that contrastive pre-training, which learns features on unlabeled source and target data and then fine-tunes on labeled source data, is competitive with strong UDA methods. However, we find that contrastive pre-training does not learn domain-invariant features, diverging from conventional UDA intuitions. We show theoretically that contrastive pre-training can learn features that vary subtantially across domains but still generalize to the target domain, by disentangling domain and class information. Our results suggest that domain invariance is not necessary for UDA. We empirically validate our theory on benchmark vision datasets.","pre-training, representation learning, domain adaptation, contrastive learning, spectral graph theory",kshen6@cs.stanford.edu,https://drive.google.com/open?id=1kD7UTKNDk0y0vn1n6z4PlsgbokYejzGU,Long talk,Main conference,,https://arxiv.org/abs/2204.00570,,
6/27/2022 10:03:35,architsh@stanford.edu,A State-Distribution Matching Approach to Non-Episodic Reinforcement Learning,"Archit Sharma*, Rehaan Ahmad*, Chelsea Finn","While reinforcement learning (RL) provides a framework for learning through trial and error, translating RL algorithms into the real world has remained challenging. A major hurdle to real-world application arises from the development of algorithms in an episodic setting where the environment is reset after every trial, in contrast with the continual and non-episodic nature of the real-world encountered by embodied agents such as humans and robots. Prior works have considered an alternating approach where a forward policy learns to solve the task and the backward policy learns to reset the environment, but what initial state distribution should the backward policy reset the agent to? Assuming access to a few demonstrations, we propose a new method, MEDAL, that trains the backward policy to match the state distribution in the provided demonstrations. This keeps the agent close to the task-relevant states, allowing for a mix of easy and difficult starting states for the forward policy. Our experiments show that MEDAL matches or outperforms prior methods on three sparse-reward continuous control tasks from the EARL benchmark, with 40% gains on the hardest task, while making fewer assumptions than prior works.","reinforcement learning, continual learning, adversarial learning",architsh@stanford.edu,https://drive.google.com/open?id=1z1rH_-U8ERkmw1Tna6a24o9KAN8le9Te,,Main conference,https://sites.google.com/view/medal-arl/home,https://arxiv.org/abs/2205.05212,,
6/27/2022 15:04:50,merty@stanford.edu,Meaningfully debugging model mistakes using conceptual counterfactual explanations,"Abubakar Abid, Mert Yuksekgonul, James Zou","Understanding and explaining the mistakes made by trained models is critical to many machine learning objectives, such as improving robustness, addressing concept drift, and mitigating biases. However, this is often an ad hoc process that involves manually looking at the model’s mistakes on many test samples and guessing at the underlying reasons for those incorrect predictions. In this paper, we propose a systematic approach, conceptual counterfactual explanations (CCE), that explains why a classifier makes a mistake on a particular test sample(s) in terms of human-understandable concepts (e.g. this zebra is misclassified as a dog because of faint stripes). We base CCE on two prior ideas: counterfactual explanations and concept activation vectors, and validate our approach on well-known pretrained models, showing that it explains the models’ mistakes meaningfully. In addition, for new models trained on data with spurious correlations, CCE accurately identifies the spurious correlation as the cause of model mistakes from a single misclassified test sample. On two challenging medical applications, CCE generated useful insights, confirmed by clinicians, into biases and mistakes the model makes in real-world settings.","interpretability, counterfactual explanations, concept-based explanations, reliable machine learning",merty@stanford.edu,https://drive.google.com/open?id=17L_M0FdsescoYnfj1t9veafa6lkh-Lra,,Main conference,,https://arxiv.org/abs/2106.12723,,
6/27/2022 17:00:22,mfchen@stanford.edu,Perfectly Balanced: Improving Transfer and Robustness of Supervised Contrastive Learning,"Mayee Chen*, Dan Fu*, Avanika Narayan, Michael Zhang, Zhao Song, Kayvon Fatahalian, Chris Ré","An ideal learned representation should display transferability and robustness. Supervised contrastive learning (SupCon) is a promising method for training accurate models, but produces representations that do not capture these properties due to class collapse -- when all points in a class map to the same representation. Recent work suggests that ""spreading out"" these representations improves them, but the precise mechanism is poorly understood. We argue that creating spread alone is insufficient for better representations, since spread is invariant to permutations within classes. Instead, both the correct degree of spread and a mechanism for breaking this invariance are necessary. We first prove that adding a weighted class-conditional InfoNCE loss to SupCon controls the degree of spread. Next, we study three mechanisms to break permutation invariance: using a constrained encoder, adding a class-conditional autoencoder, and using data augmentation. We show that the latter two encourage clustering of latent subclasses under more realistic conditions than the former. Using these insights, we show that adding a properly-weighted class-conditional InfoNCE loss and a class-conditional autoencoder to SupCon achieves 11.1 points of lift on coarse-to-fine transfer across 5 standard datasets and 4.7 points on worst-group robustness on 3 datasets, setting state-of-the-art on CelebA by 11.5 points.","contrastive learning, transfer learning, robustness","mfchen@stanford.edu, danfu@cs.stanford.edu",https://drive.google.com/open?id=1r-_Imi2sW1r34qefI3SF1t5bDWXei3ho,,Main conference,https://github.com/HazyResearch/thanos-code,https://arxiv.org/abs/2204.07596,https://hazyresearch.stanford.edu/blog/2022-04-19-contrastive-2,https://www.youtube.com/watch?v=G3yLSBSCUUw
6/27/2022 22:55:24,tianheyu@cs.stanford.edu,How to Leverage Unlabeled Data in Offline Reinforcement Learning,"Tianhe Yu*, Aviral Kumar*, Yevgen Chebotar, Karol Hausman, Chelsea Finn, Sergey Levine","Offline reinforcement learning (RL) can learn control policies from static datasets but, like standard RL methods, it requires reward annotations for every transition. In many cases, labeling large datasets with rewards may be costly, especially if those rewards must be provided by human labelers, while collecting diverse unlabeled data might be comparatively inexpensive. How can we best leverage such unlabeled data in offline RL? One natural
solution is to learn a reward function from the labeled data and use it to label the unlabeled data. In this paper, we find that, perhaps surprisingly, a much simpler method that simply applies zero rewards to unlabeled data leads to effective data sharing both in theory and in practice, without learning any reward model at all. While this approach might seem strange (and incorrect) at first, we provide extensive theoretical and empirical analysis
that illustrates how it trades off reward bias, sample complexity and distributional shift, often leading to good results. We characterize conditions under which this simple strategy is effective, and further show that extending it with a simple reweighting approach can further alleviate the bias introduced by using incorrect reward labels. Our experiments confirm these findings in simulated robotic locomotion, navigation, and manipulation settings.","Offline RL, Deep RL",tianheyu@cs.stanford.edu,https://drive.google.com/open?id=1CExbdD5Si5MfXzNxLcvIqB0-02di4qH9,,Main conference,,https://arxiv.org/abs/2202.01741,,
6/28/2022 13:27:46,phend@stanford.edu,Integrating Reward Maximization and Population Estimation: Sequential Decision-Making for Internal Revenue Service Audit Selection,"Peter Henderson, Ben Chugg, Brandon Anderson, Kristen Altenburger, Alex Turk, John Guyton, Jacob Goldin, Daniel E. Ho","We introduce a new setting, optimize-and-estimate structured bandits. Here, a policy must select a batch of arms, each characterized by its own context, that would allow it to both maximize reward and maintain an accurate (ideally unbiased) population estimate of the reward. This setting is inherent to many public and private sector applications and often requires handling delayed feedback, small data, and distribution shifts. We demonstrate its importance on real data from the United States Internal Revenue Service (IRS). The IRS performs yearly audits of the tax base. Two of its most important objectives are to identify suspected misreporting and to estimate the ""tax gap"" - the global difference between the amount paid and true amount owed. We cast these two processes as a unified optimize-and-estimate structured bandit. We provide a novel mechanism for unbiased population estimation that achieves rewards comparable to baseline approaches. This approach has the potential to improve audit efficacy, while maintaining policy-relevant estimates of the tax gap. This has important social consequences given that the current tax gap is estimated at nearly half a trillion dollars. We suggest that this problem setting is fertile ground for further research and we highlight its interesting challenges. ","bandits,real world ml,sampling",phend@cs.stanford.edu,https://drive.google.com/open?id=1zynFLjP1au7qyGIQRMYEB3AJA69NVmkY,,Adaptive Experimental Design and Active Learning in the Real World Workshop,,https://arxiv.org/abs/2204.11910,,
6/28/2022 15:45:54,jiaming.tsong@gmail.com,A General Recipe for Likelihood-free Bayesian Optimization,"Jiaming Song, Lantao Yu, Willie Neiswanger, Stefano Ermon","The acquisition function, a critical component in Bayesian optimization (BO), can often be written as the expectation of a utility function under a surrogate model. However, to ensure that acquisition functions are tractable to optimize, restrictions must be placed on the surrogate model and utility function. To extend BO to a broader class of models and utilities, we propose likelihood-free BO (LFBO), an approach based on likelihood-free inference.

LFBO directly models the acquisition function without having to separately perform inference with a probabilistic surrogate model. We show that computing the acquisition function in LFBO can be reduced to optimizing a weighted classification problem, where the weights correspond to the utility being chosen. LFBO outperforms various state-of-the-art black-box optimization methods on several real-world optimization problems. LFBO can also effectively leverage composite structures of the objective function, which further improves its regret by several orders of magnitude.","bayesian optimization, likelihood-free inference",jiaming.tsong@gmail.com,https://drive.google.com/open?id=1cIAUeP5Xmz8jarqW4yK51E2elySIzbFJ,,Main conference,https://lfbo-ml.github.io/,https://arxiv.org/abs/2206.13035,,
6/29/2022 9:32:06,atticusg@stanford.edu,Inducing Causal Structure for Interpretable Neural Networks,"Atticus Geiger*, Zhengxuan Wu*, Hanson Lu*, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah D. Goodman, Christopher Potts","In many areas, we have well-founded insights about causal structure that would be useful to bring into our trained models while still allowing them to learn in a data-driven fashion. To achieve this, we present the new method of interchange intervention training (IIT). In IIT, we (1) align variables in a causal model (e.g., a deterministic program or Bayesian network) with representa- tions in a neural model and (2) train the neural model to match the counterfactual behavior of the causal model on a base input when aligned representations in both models are set to be the value they would be for a source input. IIT is fully differentiable, flexibly combines with other objectives, and guarantees that the target causal model is a causal abstraction of the neural model when its loss is zero. We evaluate IIT on a struc- tural vision task (MNIST-PVR), a navigational language task (ReaSCAN), and a natural language inference task (MQNLI). We compare IIT against multi-task training objectives and data augmen- tation. In all our experiments, IIT achieves the best results and produces neural models that are more interpretable in the sense that they more successfully realize the target causal model.","Causality, interpretability, ",atticusg@gmail.com,https://drive.google.com/open?id=1onX1ngu4rxxw1Sz20zCSnk8kt_l7MdA1,,Main conference,https://github.com/frankaging/Interchange-Intervention-Training,https://arxiv.org/abs/2112.00826,,
6/29/2022 20:05:57,kawin@stanford.edu,Understanding Dataset Difficulty with V-Usable Information,"Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta","
Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty -- w.r.t. a model V -- as the lack of V-usable information (Xu et al., 2019), where a lower value indicates a more difficult dataset for V. We further introduce pointwise V-information (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, V-usable information and PVI also permit the converse: for a given model V, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.","dataset,interpretability,data-centric AI,information theory",kawin@stanford.edu,https://drive.google.com/open?id=1sJCC6fFMx0amVu6-ClsJbyeq6RpFTLqN,long talk,Main conference,,https://arxiv.org/abs/2110.08420,,
6/30/2022 9:27:59,rschaef@cs.stanford.edu,Streaming Inference for Infinite Feature Models,"Rylan Schaeffer, Yilun Du, Gabrielle Kaili-May Liu, Ila Rani Fiete","Unsupervised learning from a continuous stream of data is arguably one of the most common and most challenging problems facing intelligent agents. One class of unsupervised models, collectively termed \textit{feature models}, attempts unsupervised discovery of latent features underlying the data and includes common models such as PCA, ICA, and NMF. However, if the data arrives in a continuous stream, determining the number of features is a significant challenge, and the number may grow with time. In this work, we make feature models significantly more applicable to streaming data by imbuing them with the ability to create new features, online, in a probabilistic and principled manner. To achieve this, we derive a novel recursive form of the Indian Buffet Process, which we term the \textit{Recursive IBP} (R-IBP). We demonstrate that R-IBP can be be used as a prior for feature models to efficiently infer a posterior over an unbounded number of latent features, with quasilinear average time complexity and logarithmic average space complexity. We compare R-IBP to existing sampling and variational baselines in two feature models (Linear Gaussian and Factor Analysis) and demonstrate on synthetic and real data that R-IBP achieves comparable or better performance in significantly less time.","variational inference, combinatorial stochastic processes, Bayesian nonparametrics",rschaef@cs.stanford.edu,https://drive.google.com/open?id=1z8F46p1AjgUoWfVsW2Gm2w0h8UaS0DTM,"Spotlight (but as far as I can tell, everything at ICML is a spotlight)",Main conference,,,,
6/30/2022 9:31:07,rschaef@stanford.edu,No Free Lunch from Deep Learning in Neuroscience: A Case Study through Models of the Entorhinal-Hippocampal Circuit,"Rylan Schaeffer, Mikail Khona, Ila Rani Fiete","Research in Neuroscience, as in many scientific disciplines, is undergoing a renaissance based on deep learning. Unique to Neuroscience, deep learning models can be used not only as a tool but interpreted as models of the brain. The central claims of recent deep learning-based models of brain circuits are that they make novel predictions about neural phenomena or shed light on the fundamental functions being optimized. We show, through the case-study of grid cells in the entorhinal-hippocampal circuit, that one often gets neither. We begin by reviewing the principles of grid cell mechanism and function obtained from analytical and first-principles modeling efforts, then rigorously examine the claims of deep learning models of grid cells. Using large-scale hyperparameter sweeps and theory-driven experimentation, we demonstrate that the results of such models may be more strongly driven by particular, non-fundamental, and post-hoc implementation choices than fundamental truths about neural circuits or the loss function(s) they might optimize. Finally, we discuss why these models cannot be expected to produce accurate models of the brain without the addition of substantial amounts of inductive bias, an informal No Free Lunch result for Neuroscience. In conclusion, caution and consideration, together with biological knowledge, are warranted in building and interpreting deep learning models in Neuroscience.","deep learning, neuroscience",rschaef@cs.stanford.edu,https://drive.google.com/open?id=1FazTVwNnAXa68xilr3OOh6WafklVUcj4,,ICML Workshop AI 4 Science (http://www.ai4science.net/icml22/index.html),,,,
7/2/2022 12:15:48,phend@stanford.edu,Self-Destructing Models: Increasing the Costs of Harmful Dual Uses in Foundation Models,"Eric Mitchell*, Peter Henderson*, Christopher D. Manning, Dan Jurafsky, Chelsea Finn","A growing ecosystem of large, open-source foundation models has reduced the labeled data and technical expertise necessary to apply machine learning to many new problems. Yet foundation models pose a clear dual-use risk, indiscriminately reducing the costs of building both harmful and benign machine learning systems. To mitigate this risk, we propose the task blocking paradigm, in which foundation models are trained with an additional mechanism to impede adaptation to harmful tasks while retaining good performance on desired tasks. We call the resulting models self-destructing models} inspired by mechanisms that prevent adversaries from using tools for harmful purposes. We present an algorithm for training self-destructing models leveraging techniques from meta-learning and adversarial learning, showing that it can largely prevent a BERT-based model from learning to perform gender identification without harming the model's ability to perform profession classification. We conclude with a discussion of future directions.","foundation models, ai safety, meta-learning",phend@stanford.edu,https://drive.google.com/open?id=1xFq-khP5iwg-jUgJRwO-kXnWNn5XECok,,"First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022 and New Frontiers in Adversarial Machine Learning (AdvML Frontiers @ ICML 2022)",,,,
7/3/2022 13:05:17,mzhang20@stanford.edu,Correct-N-Contrast: a Contrastive Approach for Improving Robustness to Spurious Correlations,"Michael Zhang, Nimit S. Sohoni, Hongyang R. Zhang, Chelsea Finn, Christopher Ré","Spurious correlations pose a major challenge for robust machine learning. Models trained with empirical risk minimization (ERM) may learn to rely on correlations between class labels and spurious attributes, leading to poor performance on data groups without these correlations. This is challenging to address when the spurious attribute labels are unavailable. To improve worst-group performance on spuriously correlated data without training attribute labels, we propose Correct-N-Contrast (CNC), a contrastive approach to directly learn representations robust to spurious correlations. As ERM models can be good spurious attribute predictors, CNC works by (1) using a trained ERM model’s outputs to identify samples with the same class but dissimilar spurious features, and (2) training a robust model with contrastive learning to learn similar representations for these samples. To support CNC, we introduce new connections between worst-group error and a representation alignment loss that CNC aims to minimize. We empirically observe that worst-group error closely tracks with alignment loss, and prove that the alignment loss over a class helps upper-bound the class's worst-group vs. average error gap. On popular benchmarks, CNC reduces alignment loss drastically, and achieves state-of-the-art worst-group accuracy by 3.6% average absolute lift. CNC is also competitive with oracle methods that require group labels.","spurious correlations, robustness, contrastive learning",mzhang@cs.stanford.edu,https://drive.google.com/open?id=18mKZNTrBAfJ9eO6lCN31pj5iO4fUPtFG,Long Talk (Oral),Main conference,,https://arxiv.org/pdf/2203.01517.pdf,,
7/3/2022 13:14:20,mzhang20@stanford.edu,Contrastive Adapters for Foundation Model Group Robustness,"Michael Zhang, Christopher Ré","While large pretrained foundation models (FMs) have shown remarkable zero-shot classification robustness to dataset-level distribution shifts, their robustness to group shifts is relatively underexplored. We study this problem, and first find that popular FMs such as CLIP may not be robust to various group shifts. On prior robustness benchmarks, they achieve up to an 80.7 percentage point (pp) gap between average and worst-group accuracy. Unfortunately, current methods to improve robustness require retraining, which can be prohibitively expensive for large FMs. We find existing ways to efficiently improve large model inference, e.g, by training adapters (lightweight MLPs) on top of FM embeddings, can also hurt group robustness compared to zero-shot. We thus propose a first adapter training method designed to improve FM robustness to group shifts. While prior work only trains adapters with class labels, we add a contrastive objective to explicitly learn similar embeddings for initially dissimilar FM embeddings. Across the same benchmarks, contrastive adapting effectively and efficiently improves group robustness, raising worst-group accuracy by 16.0 to 56.0 pp over zero-shot without any FM finetuning. Beyond FM robustness, contrastive adapting achieves near-state-of-the-art robustness on Waterbirds and CelebA, while only training 1% of other methods’ model parameters.","robustness, foundation models, lightweight tuning, adapters",mzhang@cs.stanford.edu,https://drive.google.com/open?id=1Y1GZDYh6k2wxMnB13uuLlEtfiEMmdKPG,,"Workshop on Spurious Correlations, Invariance, and Stability",,https://www.dropbox.com/home?preview=contrastive_adapter_scis2022.pdf,,
7/4/2022 1:16:40,huaxiu@cs.stanford.edu,Improving Out-of-Distribution Robustness via Selective Augmentation,"Huaxiu Yao*, Yu Wang*, Sai Li, Linjun Zhang, Weixin Liang, James Zou, Chelsea Finn","Machine learning algorithms typically assume that training and test examples are drawn from the same distribution. However, distribution shift is a common problem in real-world applications and can cause models to perform dramatically worse at test time. In this paper, we specifically consider the problems of subpopulation shifts (e.g., imbalanced data) and domain shifts. While prior works often seek to explicitly regularize internal representations or predictors of the model to be domain invariant, we instead aim to learn invariant predictors without restricting the model’s internal representations or predictors. This leads to a simple mixup-based technique which learns invariant predictors via selective augmentation called LISA. LISA selectively interpolates samples either with the same labels but different domains or with the same domain but different labels. Empirically, we study the effectiveness of LISA on nine benchmarks ranging from subpopulation shifts to domain shifts, and we find that LISA consistently outperforms other state-of-the-art methods and leads to more invariant predictors. We further analyze a linear setting and theoretically show how LISA leads to a smaller worst-group error. Code is released in https://github.com/huaxiuyao/LISA","out-of-distribution robustness, domain generalization, spurious correlation, distribution shifts, selective augmentation",huaxiu@cs.stanford.edu,https://drive.google.com/open?id=1f4-NUzKYXpRA9C0ScXsQSYBrK6TjDNo5,,Main conference,,https://uploads.strikinglycdn.com/files/acf2ccd0-aea2-46a6-86e5-152234c4036d/LISA_ICML22.pdf,,https://www.youtube.com/watch?v=jaLkGVoun_4
7/4/2022 6:14:10,lnardi@stanford.edu,Joint Entropy Search For Maximally-Informed Bayesian Optimization,"Carl Hvarfner, Frank Hutter, Luigi Nardi","Information-theoretic Bayesian optimization techniques have become popular for optimizing expensive-to-evaluate black-box functions due to their non-myopic qualities. Entropy Search and Predictive Entropy Search both consider the entropy over the optimum in the input space, while the recent Max-value Entropy Search considers the entropy over the optimal value in the output space. We propose Joint Entropy Search (JES), a novel information-theoretic acquisition function that considers an entirely new quantity, namely the entropy over the joint optimal probability density over both input and output space. To incorporate this information, we consider the reduction in entropy from conditioning on fantasized optimal input/output pairs. The resulting approach primarily relies on standard GP machinery and  removes complex approximations typically associated with information-theoretic methods. With minimal computational overhead, JES shows superior decision-making, and yields state-of-the-art performance for information-theoretic approaches across a wide suite of tasks. As a light-weight approach with superior results, JES provides a new go-to acquisition function for Bayesian optimization. ","Bayesian Optimization, Hyperparameter Optimization, Entropy Search",lnardi@stanford.edu,https://drive.google.com/open?id=1oB2-w_QZTc9OGeZxbhDWIBQ1jRACz_AE,,Adaptive Experimental Design and Active Learning in the Real World (ReALML) workshop,https://arxiv.org/abs/2206.04771,https://arxiv.org/pdf/2206.04771.pdf,,