Timestamp,Email Address,Title,"Authors (full name, comma separated)",Abstract,key words,Point of contact email address,Image To Represent Paper,"Award Nominations (if any, comma separated)","Venue (main conference / a particular workshop / etc). If the paper is presented in a workshop, please list the workshop name.",Link to project website,Link to paper,Link to blog post (if any),"Link to public video (e.g. YouTube, if any)"
6/14/2022 17:43:43,borisi@stanford.edu,Propagating State Uncertainty Through Trajectory Forecasting,"Boris Ivanovic, Yifeng (Richard) Lin, Shubham Shrivastava, Punarjay Chakravarty, Marco Pavone","Uncertainty pervades through the modern robotic autonomy stack, with nearly every component (e.g., sensors, detection, classification, tracking, behavior prediction) producing continuous or discrete probabilistic distributions. Trajectory forecasting, in particular, is surrounded by uncertainty as its inputs are produced by (noisy) upstream perception and its outputs are predictions that are often probabilistic for use in downstream planning. However, most trajectory forecasting methods do not account for upstream uncertainty, instead taking only the most-likely values. As a result, perceptual uncertainties are not propagated through forecasting and predictions are frequently overconfident. To address this, we present a novel method for incorporating perceptual state uncertainty in trajectory forecasting, a key component of which is a new statistical distance-based loss function which encourages predicting uncertainties that better match upstream perception. We evaluate our approach both in illustrative simulations and on large-scale, real-world data, demonstrating its efficacy in propagating perceptual state uncertainty through prediction and producing more calibrated predictions.","uncertainty propagation, trajectory forecasting, autonomous vehicle perception",borisi@stanford.edu,https://drive.google.com/open?id=1ZDIvrDaorS9c_qPeolHaVPgOkB0_NqM_,,Main conference,,https://arxiv.org/abs/2110.03267,,
6/14/2022 20:05:56,belkhale@stanford.edu,Balancing Efficiency and Comfort in Robot-Assisted Bite Transfer,"Suneel Belkhale, Ethan K. Gordon, Yuxiao Chen, Siddhartha Srinivasa, Tapomayukh Bhattacharjee, Dorsa Sadigh","Robot-assisted feeding in household environments is challenging because it requires robots to generate trajectories that effectively bring food items of varying shapes and sizes into the mouth while making sure the user is comfortable. Our key insight is that in order to solve this challenge, robots must balance the efficiency of feeding a food item with the comfort of each individual bite. We formalize comfort and efficiency as heuristics to incorporate in motion planning. We present an approach based on heuristics-guided bi-directional Rapidly-exploring Random Trees (h-BiRRT) that selects bite transfer trajectories of arbitrary food item geometries and shapes using our developed bite efficiency and comfort heuristics and a learned constraint model. Real-robot evaluations show that optimizing both comfort and efficiency significantly outperforms a fixed-pose based method, and users preferred our method significantly more than that of a method that maximizes only user comfort. Videos and Appendices are found on our website: https://sites.google.com/view/comfortbitetransfer-icra22/home","motion planning, human robot interaction, assistive feeding",belkhale@stanford.edu,https://drive.google.com/open?id=122cjVE2JjnOOZgJ-z2tE1sJo0MF2Mj2j,,Main conference,https://sites.google.com/view/comfortbitetransfer-icra22/home,https://arxiv.org/abs/2111.11401,,
6/14/2022 23:06:23,jiachen_li@stanford.edu,Grouptron: Dynamic Multi-Scale Graph Convolutional Networks for Group-Aware Dense Crowd Trajectory Forecasting,"Rui Zhou, Hongyu Zhou, Huidong Gao, Masayoshi Tomizuka, Jiachen Li, Zhuo Xu","Accurate, long-term forecasting of pedestrian trajectories in highly dynamic and interactive scenes is a long-standing challenge. Recent advances in using data-driven approaches have achieved significant improvements in terms of prediction accuracy. However, the lack of group-aware analysis has limited the performance of forecasting models. This is especially nonnegligible in highly crowded scenes, where pedestrians are moving in groups and the interactions between groups are extremely complex and dynamic. In this paper, we present Grouptron, a multi-scale dynamic forecasting framework that leverages pedestrian group detection and utilizes individual-level, group-level and scene-level information for better understanding and representation of the scenes. Our approach employs spatio-temporal clustering algorithms to identify pedestrian groups, creates spatio-temporal graphs at the individual, group, and scene levels. It then uses graph neural networks to encode dynamics at different scales and aggregate the embeddings for trajectory prediction. We conducted extensive comparisons and ablation experiments to demonstrate the effectiveness of our approach. Our method achieves 9.3% decrease in final displacement error (FDE) compared with state-of-the-art methods on ETH/UCY benchmark datasets, and 16.1% decrease in FDE in more crowded scenes where extensive human group interactions are more frequently present.","Trajectory prediction, graph neural network, interaction modeling, human behaviors",jiachen_li@stanford.edu,https://drive.google.com/open?id=1mQlUGgp28IAvUdw_70cBPua89_pcPSj0,,Main conference,,https://arxiv.org/abs/2109.14128,,
6/14/2022 23:11:52,jiachen_li@stanford.edu,Important Object Identification with Semi-Supervised Learning for Autonomous Driving,"Jiachen Li, Haiming Gang, Hengbo Ma, Masayoshi Tomizuka, Chiho Choi","Accurate identification of important objects in the scene is a prerequisite for safe and high-quality decision making and motion planning of intelligent agents (e.g., autonomous vehicles) that navigate in complex and dynamic environments. Most existing approaches attempt to employ attention mechanisms to learn importance weights associated with each object indirectly via various tasks (e.g., trajectory prediction), which do not enforce direct supervision on the importance estimation. In contrast, we tackle this task in an explicit way and formulate it as a binary classification (""important"" or ""unimportant"") problem. We propose a novel approach for important object identification in egocentric driving scenarios with relational reasoning on the objects in the scene. Besides, since human annotations are limited and expensive to obtain, we present a semi-supervised learning pipeline to enable the model to learn from unlimited unlabeled data. Moreover, we propose to leverage the auxiliary tasks of ego vehicle behavior prediction to further improve the accuracy of importance estimation. The proposed approach is evaluated on a public egocentric driving dataset (H3D) collected in complex traffic scenarios. A detailed ablative study is conducted to demonstrate the effectiveness of each model component and the training strategy. Our approach also outperforms rule-based baselines by a large margin.","Autonomous driving, scene understanding, relational reasoning, graph neural network",jiachen_li@stanford.edu,https://drive.google.com/open?id=1QL5516gwDfzPZfKeMn1jR9B1c3YglC0S,,Main conference,,https://arxiv.org/abs/2203.02634,,
6/14/2022 23:21:33,wangzih@stanford.edu,Learning from Imperfect Demonstrations via Adversarial Confidence Transfer,"Zhangjie Cao*, Zihan Wang*, Dorsa Sadigh","Existing learning from demonstration algorithms usually assume access to expert demonstrations. However, this assumption is limiting in many real-world applications since the collected demonstrations may be suboptimal or even consist of failure cases. We therefore study the problem of learning from imperfect demonstrations by learning a confidence predictor. Specifically, we rely on demonstrations along with their confidence values from a different correspondent environment (source environment) to learn a confidence predictor for the environment we aim to learn a policy in (target environment -- where we only have unlabeled demonstrations.) We learn a common latent space through adversarial distribution matching of multi-length partial trajectories to enable the transfer of confidence across source and target environments. The learned confidence reweights the demonstrations to enable learning more from informative demonstrations and discarding the irrelevant ones. Our experiments in three simulated environments and a real robot reaching task demonstrate that our approach learns a policy with the highest expected return.","Robotics, Machine Learning",caozj@cs.stanford.edu,https://drive.google.com/open?id=1oGPj_hTAcyOBvY4sOydPtjZIayl9xfAk,,Main conference,https://sites.google.com/view/adversarialconfidencetransfer,https://iliad.stanford.edu/pdfs/publications/cao2022learning.pdf,,https://www.youtube.com/watch?v=NbC4mF5g53A
6/14/2022 23:23:26,wangzih@stanford.edu,Weakly Supervised Correspondence Learning,"Zihan Wang*, Zhangjie Cao*, Yilun Hao, Dorsa Sadigh","Correspondence learning is a fundamental prob- lem in robotics, which aims to learn a mapping between state, action pairs of agents of different dynamics or em- bodiments. However, current correspondence learning methods either leverage strictly paired data—which are often difficult to collect—or learn in an unsupervised fashion from unpaired data using regularization techniques such as cycle-consistency— which suffer from severe misalignment issues. We propose a weakly supervised correspondence learning approach that trades off between strong supervision over strictly paired data and unsupervised learning with a regularizer over unpaired data. Our idea is to leverage two types of weak supervision: i) temporal ordering of states and actions to reduce the compound- ing error, and ii) paired abstractions, instead of paired data, to alleviate the misalignment problem and learn a more accurate correspondence. The two types of weak supervision are easy to access in real-world applications, which simultaneously reduces the high cost of annotating strictly paired data and improves the quality of the learned correspondence. ","Robotics, Machine Learning",wangzih@stanford.edu,https://drive.google.com/open?id=1iVVV4geQGkBx1FSADa1CcJnDRZlLPA-H,,Main conference,https://sites.google.com/stanford.edu/weakly-supervised-correspond,https://iliad.stanford.edu/pdfs/publications/wang2022weakly.pdf,,https://www.youtube.com/watch?v=F5rTzT2YU7E&t=138s
6/15/2022 16:37:38,snewdick@stanford.edu,ReachBot: A Small Robot with Exceptional Reach for Rough Terrain,"Tony G. Chen, Becky Miller, Crystal Winston, Stephanie Newdick, Andrew Bylard, Marco Pavone, Mark R. Cutkosky","ReachBot is a new concept for planetary exploration, consisting of a small body and long, lightweight extending arms loaded primarily in tension. The arms are equipped with spined grippers for anchoring on rock surfaces. The design and testing of a planar prototype is presented here. Experiments with rock grasping and coordinated locomotion illustrate the advantages of low inertia passive grippers, triggered by impact and using stored mechanical energy for the internal force. Gripper design involves a trade-off among the range of possible grasp angles, maximum grasp force, required triggering force, and required reset force. The current prototype can pull with up to 8N when gripping volcanic rock, limited only by the strength of the 3D printed components. Calculations predict a maximum pull of 26N for the same spines and stronger materials.","Mobile Manipulation, Space Robotics and Automation, Climbing Robots, Grippers and Other End-Effectors, Mechanism Design",snewdick@stanford.edu,https://drive.google.com/open?id=1VgJCK-yB2qKMbEKlBBeC7dHsplV1vv9_,,"Main conference. A similar paper concerning the same concept was presented in the ""New Frontiers of Parallel Robotics"" workshop",http://bdml.stanford.edu/Main/ReachBot,,,
6/16/2022 10:11:30,takatoki@cs.stanford.edu,Grounding Predicates through Actions,"Toki Migimatsu, Jeannette Bohg","Symbols representing abstract states such as ""dish in dishwasher"" or ""cup on table"" allow robots to reason over long horizons by hiding details unnecessary for high-level planning. Current methods for learning to identify symbolic states in visual data require large amounts of labeled training data, but manually annotating such datasets is prohibitively expensive due to the combinatorial number of predicates in images. We propose a novel method for automatically labeling symbolic states in large-scale video activity datasets by exploiting known pre- and post-conditions of actions. This automatic labeling scheme only requires weak supervision in the form of an action label that describes which action is demonstrated in each video. We use our framework to train predicate classifiers to identify symbolic relationships between objects when prompted with object bounding boxes, and demonstrate that such predicate classifiers can match the performance of those trained with full supervision at a fraction of the labeling cost. We also apply our framework to an existing large-scale human activity dataset, and demonstrate the ability of these predicate classifiers trained on human data to enable closed-loop task planning in the real world.","Perception for manipulation, visual relationship detection, task planning",takatoki@cs.stanford.edu,https://drive.google.com/open?id=1YgSDSJB9Cof9hgC7IFh_V_7WHImx8bjw,,Main conference,https://sites.google.com/stanford.edu/groundingpredicates,https://arxiv.org/abs/2109.14718,,
6/16/2022 10:16:35,takatoki@cs.stanford.edu,Symbolic State Estimation with Predicates for Contact-Rich Manipulation Tasks,"Toki Migimatsu, Wenzhao Lian, Jeannette Bohg, Stefan Schaal","Manipulation tasks often require a robot to adjust its sensorimotor skills based on the state it finds itself in. Taking peg-in-hole as an example: once the peg is aligned with the hole, the robot should push the peg downwards. While high level execution frameworks such as state machines and behavior trees are commonly used to formalize such decision-making problems, these frameworks require a mechanism to detect the high-level symbolic state. Handcrafting heuristics to identify symbolic states can be brittle, and using data-driven methods can produce noisy predictions, particularly when working with limited datasets, as is common in real-world robotic scenarios. This paper proposes a Bayesian state estimation method to predict symbolic states with predicate classifiers. This method requires little training data and allows fusing noisy observations from multiple sensor modalities. We evaluate our framework on a set of real-world peg-in-hole and connector-socket insertion tasks, demonstrating its ability to classify symbolic states and to generalize to unseen tasks, outperforming baseline methods. We also demonstrate the ability of our method to improve the robustness of manipulation policies on a real robot.","Perception for manipulation, symbolic state estimation",takatoki@cs.stanford.edu,https://drive.google.com/open?id=1cA4sJj30ppaoMHyuLTJ9NOoOBf-bQCly,,Main conference,https://sites.google.com/stanford.edu/symbolicstateestimation/home?authuser=1,https://arxiv.org/abs/2203.02468,,
6/16/2022 12:20:43,chengine@stanford.edu,Vision-Only Robot Navigation in a Neural Radiance World,"Michal Adamkiewicz, Timothy Chen, Adam Caccavale, Rachel Gardner,  Preston Culbertson,  Jeannette Bohg, Mac Schwager","Neural Radiance Fields (NeRFs) have recently emerged as a powerful paradigm for the representation of natural, complex 3D scenes. Neural Radiance Fields (NeRFs) represent continuous volumetric density and RGB values in a neural network, and generate photo-realistic images from unseen camera viewpoints through ray tracing. We propose an algorithm for navigating a robot through a 3D environment represented as a NeRF using only an onboard RGB camera for localization. We assume the NeRF for the scene has been pre-trained offline, and the robot’s objective is to navigate through unoccupied space in the NeRF to reach a goal pose. We introduce a trajectory optimization algorithm that avoids collisions with high-density regions in the NeRF based on a discrete time version of differential flatness that is amenable to constraining the robot’s full pose and control inputs. We also introduce an optimization based filtering method to estimate 6DoF pose and velocities for the robot in the NeRF given only an onboard RGB camera. We combine the trajectory planner with the pose filter in an online replanning loop to give a vision-based robot navigation pipeline. We present simulation results with a quadrotor robot navigating through a jungle gym environment, the inside of a church, and Stonehenge using only an RGB camera. We also demonstrate an omnidirectional ground robot navigating through the church, requiring it to reorient to fit through a narrow gap.","Neural Radiance Fields, NeRFs, Vision-Only Navigation",chengine@stanford.edu,https://drive.google.com/open?id=1c9A09MC7wQMpzYMXhvqATJVu_OHqd2zT,,Main conference,https://mikh3x4.github.io/nerf-navigation/,https://mikh3x4.github.io/nerf-navigation/assets/NeRF_Navigation.pdf,,https://www.youtube.com/watch?v=5JjWpv9BaaE
6/16/2022 14:57:11,jingyuny@stanford.edu,Learning Periodic Tasks from Human Demonstrations,"Jingyun Yang, Junwu Zhang, Connor Settle, Akshara Rai, Rika Antonova, Jeannette Bohg","We develop a method for learning periodic tasks from visual demonstrations. The core idea is to leverage periodicity in the policy structure to model periodic aspects of the tasks. We use active learning to optimize parameters of rhythmic dynamic movement primitives (rDMPs) and propose an objective to maximize the similarity between the motion of objects manipulated by the robot and the desired motion in human video demonstrations. We consider tasks with deformable objects and granular matter whose states are challenging to represent and track: wiping surfaces with a cloth, winding cables/wires, and stirring granular matter with a spoon. Our method does not require tracking markers or manual annotations. The initial training data consists of 10-minute videos of random unpaired interactions with objects by the robot and human. We use these for unsupervised learning of a keypoint model to get task-agnostic visual correspondences. Then, we use Bayesian optimization to optimize rDMPs from a single human video demonstration within few robot trials. We present simulation and hardware experiments to validate our approach.","Visual Learning, Learning from Demonstration, Perception for Grasping and Manipulation",jingyuny@stanford.edu,https://drive.google.com/open?id=1mPQXK05o9hyKYD4QWMP9Az8dVAgItUBp,,Main conference,https://bit.ly/viptl_icra22,https://arxiv.org/abs/2109.14078,,