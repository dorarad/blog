Timestamp,Email Address,Title,"Authors (full name, comma separated)",Abstract,key words,Point of contact email address,Image To Represent Paper,"Award Nominations (if any, comma separated)","Venue (main conference / a particular workshop / etc). If the paper is presented in a workshop, please list the workshop name.",Link to project website,Link to paper,Link to blog post (if any),"Link to public video (e.g. YouTube, if any)"
6/27/2022 17:48:59,okhattab@stanford.edu,ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction,"Keshav Santhanam*, Omar Khattab*, Jon Saad-Falcon, Christopher Potts, Matei Zaharia","Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce Maize, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate Maize across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6–10x.","information retrieval, search, zero-shot IR, late interaction, ColBERT, efficiency",okhattab@stanford.edu,https://drive.google.com/open?id=13Sy05SSaw4U1CbWhamGLWHQ_D522MH5E,,Main conference,https://github.com/stanford-futuredata/ColBERT,https://arxiv.org/abs/2112.01488,,
6/29/2022 7:43:52,cgpotts@stanford.edu,Causal Distillation for Language Models,"Zhengxuan Wu,* Atticus Geiger,* Josh Rozner, Elisa Kreiss, Hanson Lu, Thomas Icard, Christopher Potts, and Noah D. Goodman.","Distillation efforts have led to language models that are more compact and efficient without serious drops in performance. The standard approach to distillation trains a student model against two objectives: a task-specific objective (e.g., language modeling) and an imitation objective that encourages the hidden states of the student model to be similar to those of the larger teacher model. In this paper, we show that it is beneficial to augment distillation with a third objective that encourages the student to imitate the causal computation process of the teacher through interchange intervention training(IIT). IIT pushes the student model to become a causal abstraction of the teacher model - a simpler model with the same causal structure. IIT is fully differentiable, easily implemented, and combines flexibly with other objectives. Compared with standard distillation of BERT, distillation via IIT results in lower perplexity on Wikipedia (masked language modeling) and marked improvements on the GLUE benchmark (natural language understanding), SQuAD (question answering), and CoNLL-2003 (named entity recognition).","causal abstraction, foundation models, model distillation",cgpotts@stanford.edu,https://drive.google.com/open?id=13QRC2AO92_cW1MDZ0MpqoVGU7eyggRpD,,Main conference,https://github.com/frankaging/Interchange-Intervention-Training,https://arxiv.org/abs/2112.00826,,
6/29/2022 9:55:35,katezhou@stanford.edu,"Deconstructing NLG Evaluation: Evaluation Practices, Assumptions, and Their Implications","Kaitlyn Zhou, Su Lin Blodgett, Adam Trischler, Hal Daumé III, Kaheer Suleman, Alexandra Olteanu","There are many ways to express similar things in text, which makes evaluating natural language generation (NLG) systems difficult. Compounding this difficulty is the need to assess varying quality criteria depending on the deployment setting. While the landscape of NLG evaluation has been well-mapped, practitioners' goals, assumptions, and constraints -- which inform decisions about what, when, and how to evaluate -- are often partially or implicitly stated, or not stated at all. Combining a formative semi-structured interview study of NLG practitioners (N=18) with a survey study of a broader sample of practitioners (N=61), we surface goals, community practices, assumptions, and constraints that shape NLG evaluations, examining their implications and how they embody ethical considerations.","NLG, NLG evaluation, fairness and inclusion, ethics",katezhou@stanford.edu,https://drive.google.com/open?id=1niEHy1HJRhDOkemGCQLy6zaw4NpcfzZn,,Main conference,,https://arxiv.org/abs/2205.06828,,
7/10/2022 20:03:51,salic@stanford.edu,Computationally Identifying Funneling and Focusing Questions in Classroom Discourse,"Sterling Alic, Dorottya Demszky, Zid Mancenido, Jing Liu, Heather Hill, Dan Jurafsky","Responsive teaching is a highly effective strategy that promotes student learning. In math classrooms, teachers might funnel students towards a normative answer or focus students to reflect on their own thinking, deepening their understanding of math concepts. When teachers focus, they treat students’ contributions as resources for collective sensemaking, and thereby significantly improve students’ achievement and confidence in mathematics. We propose the task of computationally detecting funneling and focusing questions in classroom discourse. We do so by creating and releasing an annotated dataset of 2,348 teacher utterances labeled for funneling and focusing questions, or neither. We introduce supervised and unsupervised approaches to differentiating these questions. Our best model, a supervised RoBERTa model fine-tuned on our dataset, has a strong linear correlation of .76 with human expert labels and with positive educational outcomes, including math instruction quality and student achievement, showing the model’s potential for use in automated teacher feedback tools. Our unsupervised measures show significant but weaker correlations with human labels and outcomes, and they highlight interesting linguistic patterns of funneling and focusing questions. The high performance of the supervised measure indicates its promise for supporting teachers in their instruction.","education, funneling, focusing, computational social science, ",salic@stanford.edu,https://drive.google.com/open?id=10Ls6cMktaHOO1gEKj0iz_qMLwo5b_fDA,,Innovative Use of NLP for Building Educational Applications,https://github.com/sterlingalic/funneling-focusing,https://drive.google.com/file/d/1te2LgPshLv6Ikd9Ozp4uIhck88Lqmam8/view?usp=sharing,,