Timestamp,Email Address,Title,"Authors (full name, comma separated)",Abstract,key words,Point of contact email address,Image To Represent Paper,"Award Nominations (if any, comma separated)","Venue (main conference / a particular workshop / etc). If the paper is presented in a workshop, please list the workshop name.",Link to project website,Link to paper,Link to blog post (if any),"Link to public video (e.g. YouTube, if any)"
6/14/2022 17:59:10,willshen@stanford.edu,ACID: Action-Conditional Implicit Visual Dynamics for Deformable Object Manipulation,"Bokui Shen, Zhenyu Jiang, Christopher Choy, Leonidas J. Guibas, Silvio Savarese, Anima Anandkumar, Yuke Zhu","Manipulating volumetric deformable objects in the real world, like plush toys and pizza dough, bring substantial challenges due to infinite shape variations, non-rigid motions, and partial observability. We introduce ACID, an action-conditional visual dynamics model for volumetric deformable objects based on structured implicit neural representations. ACID integrates two new techniques: implicit representations for action-conditional dynamics and geodesics-based contrastive learning. To represent deformable dynamics from partial RGB-D observations, we learn implicit representations of occupancy and flow-based forward dynamics. To accurately identify state change under large non-rigid deformations, we learn a correspondence embedding field through a novel geodesics-based contrastive loss. To evaluate our approach, we develop a simulation framework for manipulating complex deformable shapes in realistic scenes and a benchmark containing over 17,000 action trajectories with six types of plush toys and 78 variants. Our model achieves the best performance in geometry, correspondence, and dynamics predictions over existing approaches. The ACID dynamics models are successfully employed to goal-conditioned deformable manipulation tasks, resulting in a 30% increase in task success rate over the strongest baseline. For more results and information, please visit https://b0ku1.github.io/acid/ .","Deformable Manipulation, Implicit Representations, Simulation",willshen@stanford.edu,https://drive.google.com/open?id=1iNhAaWPOjQGplJMLYVg4mN4fRyVxUkkt,Best Student Paper Nomination,Main conference,https://b0ku1.github.io/acid/,https://arxiv.org/abs/2203.06856,,https://b0ku1.github.io/acid/assets/videos/acid_vid.mp4
6/14/2022 21:04:52,surajn@stanford.edu,Play it by Ear: Learning Skills amidst Occlusion through Audio-Visual Imitation Learning,"Maximilian Du*, Olivia Y. Lee*, Suraj Nair, Chelsea Finn","Humans are capable of completing a range of challenging manipulation tasks that require reasoning jointly over modalities such as vision, touch, and sound. Moreover, many such tasks are partially-observed; for example, taking a notebook out of a backpack will lead to visual occlusion and require reasoning over the history of audio or tactile information. While robust tactile sensing can be costly to capture on robots, microphones near or on a robot's gripper are a cheap and easy way to acquire audio feedback of contact events, which can be a surprisingly valuable data source for perception in the absence of vision.Motivated by the potential for sound to mitigate visual occlusion, we aim to learn a set of challenging partially-observed manipulation tasks from visual and audio inputs. Our proposed system learns these tasks by combining offline imitation learning from a modest number of tele-operated demonstrations and online finetuning using human provided interventions. In a set of simulated tasks, we find that our system benefits from using audio, and that by using online interventions we are able to improve the success rate of offline imitation learning by ~20%. Finally, we find that our system can complete a set of challenging, partially-observed tasks on a Franka Emika Panda robot, like extracting keys from a bag, with a 70% success rate, 50% higher than a policy that does not use audio.","Multi-modal Learning, Robotic Manipulation",surajn@stanford.edu,https://drive.google.com/open?id=1syT-z3dIZAPEZKg5rRukh_uQOEmoYLrB,,Main conference,https://sites.google.com/view/playitbyear/,https://arxiv.org/abs/2205.14850,,https://drive.google.com/file/d/16grT06YqC0XGSoSbwGx5k03aof73G6C3/view?usp=sharing